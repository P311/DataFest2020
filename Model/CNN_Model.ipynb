{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Model(data, wv_from_bin, batch_size=512, kernel_size=5, vec_size=200, epochs=8):\n",
    "    \n",
    "    # Preparing Text Data and Label\n",
    "    tweet, label = data['text'], data['sentiment']\n",
    "    label = pd.concat([label==0, label==1, label==-1], axis=1).astype(int)\n",
    "\n",
    "    # Tokenizing Text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweet)\n",
    "    sequences = tokenizer.texts_to_sequences(tweet)\n",
    "    max_length = max([len(s.split()) for s in tweet])\n",
    "\n",
    "    # Preparing Embedding Dictionary\n",
    "    word_index = tokenizer.word_index\n",
    "    tweets_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    embedding_index = {}\n",
    "    words = list(wv_from_bin.vocab.keys())\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            embedding_index[w] = wv_from_bin.word_vec(w)\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    # Preparing Embedding Matrix\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, vec_size))\n",
    "    for word, i in word_index.items():\n",
    "        # words.append(word)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    # Preparing Embedding Layer\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            vec_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    # Construction CNN Model Structure\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(600))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Training Model\n",
    "    model.fit(tweets_pad, label, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True, callbacks=[reduce_lr])\n",
    "    \n",
    "    # Saving Trained Model\n",
    "    model.save(\"/Users/colinwan/Desktop/DataFest2020/Models/CNN.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colinwan/Desktop/DataFest2020/Models/ipython/utils.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = \" \".join([word for word in df['text'][i].split()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 29, 200)           2492800   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 25, 300)           300300    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 21, 150)           225150    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 17, 75)            56325     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1275)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 600)               765600    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 1803      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 3,841,978\n",
      "Trainable params: 1,349,178\n",
      "Non-trainable params: 2,492,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 12483 samples, validate on 1388 samples\n",
      "Epoch 1/32\n",
      "12483/12483 [==============================] - 22s 2ms/step - loss: 0.9579 - accuracy: 0.5904 - val_loss: 0.8450 - val_accuracy: 0.6535\n",
      "Epoch 2/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.8356 - accuracy: 0.6137 - val_loss: 0.7484 - val_accuracy: 0.6686\n",
      "Epoch 3/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.7629 - accuracy: 0.6709 - val_loss: 0.7316 - val_accuracy: 0.6880\n",
      "Epoch 4/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.6597 - accuracy: 0.7159 - val_loss: 0.7283 - val_accuracy: 0.6787\n",
      "Epoch 5/32\n",
      "12483/12483 [==============================] - 22s 2ms/step - loss: 0.5412 - accuracy: 0.7827 - val_loss: 0.7455 - val_accuracy: 0.6635\n",
      "Epoch 6/32\n",
      "12483/12483 [==============================] - 19s 1ms/step - loss: 0.4168 - accuracy: 0.8440 - val_loss: 0.7849 - val_accuracy: 0.6873\n",
      "Epoch 7/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.3088 - accuracy: 0.8967 - val_loss: 0.8204 - val_accuracy: 0.6844\n",
      "Epoch 8/32\n",
      "12483/12483 [==============================] - 18s 1ms/step - loss: 0.2520 - accuracy: 0.9096 - val_loss: 0.9083 - val_accuracy: 0.6988\n",
      "Epoch 9/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.2135 - accuracy: 0.9240 - val_loss: 0.8706 - val_accuracy: 0.6931\n",
      "Epoch 10/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1931 - accuracy: 0.9264 - val_loss: 0.9179 - val_accuracy: 0.6924\n",
      "Epoch 11/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.1742 - accuracy: 0.9297 - val_loss: 0.9655 - val_accuracy: 0.6945\n",
      "Epoch 12/32\n",
      "12483/12483 [==============================] - 22s 2ms/step - loss: 0.1707 - accuracy: 0.9302 - val_loss: 0.9552 - val_accuracy: 0.6916\n",
      "Epoch 13/32\n",
      "12483/12483 [==============================] - 21s 2ms/step - loss: 0.1592 - accuracy: 0.9342 - val_loss: 0.9498 - val_accuracy: 0.6880\n",
      "Epoch 14/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1561 - accuracy: 0.9335 - val_loss: 0.9653 - val_accuracy: 0.6967\n",
      "Epoch 15/32\n",
      "12483/12483 [==============================] - 18s 1ms/step - loss: 0.1514 - accuracy: 0.9356 - val_loss: 0.9811 - val_accuracy: 0.6945\n",
      "Epoch 16/32\n",
      "12483/12483 [==============================] - 18s 1ms/step - loss: 0.1509 - accuracy: 0.9358 - val_loss: 0.9784 - val_accuracy: 0.6916\n",
      "Epoch 17/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.1482 - accuracy: 0.9376 - val_loss: 0.9840 - val_accuracy: 0.6902\n",
      "Epoch 18/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.1479 - accuracy: 0.9382 - val_loss: 0.9864 - val_accuracy: 0.6880\n",
      "Epoch 19/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1465 - accuracy: 0.9390 - val_loss: 0.9879 - val_accuracy: 0.6916\n",
      "Epoch 20/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1464 - accuracy: 0.9389 - val_loss: 0.9921 - val_accuracy: 0.6924\n",
      "Epoch 21/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1457 - accuracy: 0.9389 - val_loss: 0.9908 - val_accuracy: 0.6909\n",
      "Epoch 22/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1457 - accuracy: 0.9394 - val_loss: 0.9923 - val_accuracy: 0.6916\n",
      "Epoch 23/32\n",
      "12483/12483 [==============================] - 18s 1ms/step - loss: 0.1453 - accuracy: 0.9392 - val_loss: 0.9934 - val_accuracy: 0.6916\n",
      "Epoch 24/32\n",
      "12483/12483 [==============================] - 19s 1ms/step - loss: 0.1453 - accuracy: 0.9396 - val_loss: 0.9940 - val_accuracy: 0.6916\n",
      "Epoch 25/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1451 - accuracy: 0.9398 - val_loss: 0.9930 - val_accuracy: 0.6902\n",
      "Epoch 26/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.1451 - accuracy: 0.9398 - val_loss: 0.9934 - val_accuracy: 0.6902\n",
      "Epoch 27/32\n",
      "12483/12483 [==============================] - 19s 2ms/step - loss: 0.1450 - accuracy: 0.9398 - val_loss: 0.9941 - val_accuracy: 0.6924\n",
      "Epoch 28/32\n",
      "12483/12483 [==============================] - 21s 2ms/step - loss: 0.1450 - accuracy: 0.9400 - val_loss: 0.9945 - val_accuracy: 0.6924\n",
      "Epoch 29/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.1450 - accuracy: 0.9402 - val_loss: 0.9952 - val_accuracy: 0.6916\n",
      "Epoch 30/32\n",
      "12483/12483 [==============================] - 18s 1ms/step - loss: 0.1450 - accuracy: 0.9401 - val_loss: 0.9949 - val_accuracy: 0.6909\n",
      "Epoch 31/32\n",
      "12483/12483 [==============================] - 22s 2ms/step - loss: 0.1449 - accuracy: 0.9400 - val_loss: 0.9948 - val_accuracy: 0.6924\n",
      "Epoch 32/32\n",
      "12483/12483 [==============================] - 20s 2ms/step - loss: 0.1449 - accuracy: 0.9400 - val_loss: 0.9952 - val_accuracy: 0.6916\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load GloVe Vectors\n",
    "    wv_from_bin = utils.load_embedding_model(200)\n",
    "    # Load data file\n",
    "    data = pd.read_csv('/Users/colinwan/Desktop/DataFest2020/Training Folder/Sentiment.csv')\n",
    "    data = data[['text','sentiment']]\n",
    "    # Replayce word with class number\n",
    "    data = data.replace('Neutral', 0)\n",
    "    data = data.replace('Positive', 1)\n",
    "    data = data.replace('Negative', -1)\n",
    "    data = utils.clean_text(data)\n",
    "    # Filter out none alphabet character\n",
    "    data['text'] = data['text'].str.replace('[^A-Za-z ]+', '')\n",
    "    # Run model\n",
    "    CNN_M = CNN_Model(data, wv_from_bin, epochs=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
