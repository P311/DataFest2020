{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Model(data, wv_from_bin, batch_size=128, kernel_size=5, filters=3, max_length=1000, vec_size=200):\n",
    "    \n",
    "    # Preparing Text Data and Label\n",
    "    tweet, label = data['text'], data['sentiment']\n",
    "    \n",
    "    # Tokenizing Text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweet)\n",
    "    sequences = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "    # Preparing Embedding Dictionary\n",
    "    word_index = tokenizer.word_index\n",
    "    tweets_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    embedding_index = {}\n",
    "    words = list(wv_from_bin.vocab.keys())\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            embedding_index[w] = wv_from_bin.word_vec(w)\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    # Preparing Embedding Matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, vec_size))\n",
    "    words = []\n",
    "    for word, i in word_index.items():\n",
    "        words.append(word)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    # Preparing Embedding Layer\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            vec_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    # Construction CNN Model Structure\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(600))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "    filepath = \"/Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-{epoch:02d}-{loss:0.3f}-{mean_squared_error:0.3f}-{val_loss:0.3f}-{val_mean_squared_error:0.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Training Model\n",
    "    model.fit(tweets_pad, label, batch_size=batch_size, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
    "    \n",
    "    # Saving Trained Model\n",
    "    model.save(\"/Users/colinwan/Desktop/DataFest2020/Models/CNN.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 200)         2492800   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 3)            3003      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 992, 300)          4800      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 988, 150)          225150    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 984, 75)           56325     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               44280600  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 601       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 47,063,279\n",
      "Trainable params: 44,570,479\n",
      "Non-trainable params: 2,492,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 12483 samples, validate on 1388 samples\n",
      "Epoch 1/8\n",
      "12483/12483 [==============================] - 1090s 87ms/step - loss: 0.6496 - mean_squared_error: 0.6496 - val_loss: 0.5769 - val_mean_squared_error: 0.5769\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.64958, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-01-0.650-0.650-0.577-0.577.hdf5\n",
      "Epoch 2/8\n",
      "12483/12483 [==============================] - 1099s 88ms/step - loss: 0.5401 - mean_squared_error: 0.5401 - val_loss: 0.5640 - val_mean_squared_error: 0.5640\n",
      "\n",
      "Epoch 00002: loss improved from 0.64958 to 0.54007, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-02-0.540-0.540-0.564-0.564.hdf5\n",
      "Epoch 3/8\n",
      "12483/12483 [==============================] - 1190s 95ms/step - loss: 0.5168 - mean_squared_error: 0.5168 - val_loss: 0.5114 - val_mean_squared_error: 0.5114\n",
      "\n",
      "Epoch 00003: loss improved from 0.54007 to 0.51680, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-03-0.517-0.517-0.511-0.511.hdf5\n",
      "Epoch 4/8\n",
      "12483/12483 [==============================] - 1615s 129ms/step - loss: 0.4941 - mean_squared_error: 0.4941 - val_loss: 0.4902 - val_mean_squared_error: 0.4902\n",
      "\n",
      "Epoch 00004: loss improved from 0.51680 to 0.49408, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-04-0.494-0.494-0.490-0.490.hdf5\n",
      "Epoch 5/8\n",
      "12483/12483 [==============================] - 2228s 178ms/step - loss: 0.4851 - mean_squared_error: 0.4851 - val_loss: 0.4542 - val_mean_squared_error: 0.4542\n",
      "\n",
      "Epoch 00005: loss improved from 0.49408 to 0.48512, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-05-0.485-0.485-0.454-0.454.hdf5\n",
      "Epoch 6/8\n",
      "12483/12483 [==============================] - 1187s 95ms/step - loss: 0.4775 - mean_squared_error: 0.4775 - val_loss: 0.5173 - val_mean_squared_error: 0.5173\n",
      "\n",
      "Epoch 00006: loss improved from 0.48512 to 0.47748, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-06-0.477-0.477-0.517-0.517.hdf5\n",
      "Epoch 7/8\n",
      "12483/12483 [==============================] - 1156s 93ms/step - loss: 0.4647 - mean_squared_error: 0.4647 - val_loss: 0.4556 - val_mean_squared_error: 0.4556\n",
      "\n",
      "Epoch 00007: loss improved from 0.47748 to 0.46473, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-07-0.465-0.465-0.456-0.456.hdf5\n",
      "Epoch 8/8\n",
      "12483/12483 [==============================] - 1182s 95ms/step - loss: 0.4608 - mean_squared_error: 0.4608 - val_loss: 0.4857 - val_mean_squared_error: 0.4857\n",
      "\n",
      "Epoch 00008: loss improved from 0.46473 to 0.46076, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/CNN/4cnn-08-0.461-0.461-0.486-0.486.hdf5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    wv_from_bin = utils.load_embedding_model(200)\n",
    "    data = pd.read_csv('/Users/colinwan/Desktop/DataFest2020/Training Folder/Sentiment.csv')\n",
    "    data = data[['text','sentiment']]\n",
    "    data = data.replace('Neutral', 0)\n",
    "    data = data.replace('Positive', 1)\n",
    "    data = data.replace('Negative', -1)\n",
    "    data = utils.clean_text(data)\n",
    "    data['text'] = data['text'].str.replace('[^A-Za-z ]+', '')\n",
    "    CNN_Model(data, wv_from_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
