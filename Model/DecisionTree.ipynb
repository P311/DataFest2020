{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import tensorflow\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def DecisionTree(data, batch_size):\n",
    "    \n",
    "    # Preparing Text Data and Label\n",
    "    tweet, label = data['text'], data['sentiment']\n",
    "    \n",
    "    # Tokenizing Text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweet)\n",
    "    sequences = tokenizer.texts_to_sequences(tweet)\n",
    "    tweets_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Model\n",
    "    clf = DecisionTreeClassifier(max_depth=25)\n",
    "    batch_size = len(tweets_pad)\n",
    "    i = 1\n",
    "    \n",
    "    # Data\n",
    "    training_set_X = tweets_pad\n",
    "    training_set_y = label\n",
    "    \n",
    "    # Apply TF-IDF transformation\n",
    "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "    transformer.fit(training_set_X)\n",
    "    tfidf = transformer\n",
    "    training_set_X = tfidf.transform(training_set_X)\n",
    "    \n",
    "    # Traning the model\n",
    "    filepath = \"/Users/BabeChris/Desktop/4th/STA414/A1/DataFest2020/checkpoint/DecisionTree/4cnn-{epoch:02d}-{loss:0.3f}-{mean_squared_error:0.3f}-{val_loss:0.3f}-{val_mean_squared_error:0.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "    print(clf.summary())\n",
    "    \n",
    "    clf.fit(training_set_X, training_set_y, batch_size=batch_size, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
    "    \n",
    "    return clf\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{'lazy', 'fox', 'jumped', 'the', 'dog', 'quick', 'brown', 'over'}\n",
      "[7, 4, 6, 3, 5, 7, 7, 3, 5]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "print(words)\n",
    "from keras.preprocessing.text import one_hot\n",
    "# integer encode the document\n",
    "result1 = one_hot(text, round(vocab_size*1.3))\n",
    "result2 = tensorflow.keras.utils.to_categorical(one_hot(text, round(vocab_size*1.3)))\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 1]\n",
      " [5 6]\n",
      " [7 1]\n",
      " [8 0]]\n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode documents\n",
    "sequences = t.texts_to_sequences(docs)\n",
    "tweets_pad = pad_sequences(sequences, maxlen=2, padding='post')\n",
    "print(tweets_pad)\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(tweet):\n",
    "    uni_feature_vector = []\n",
    "    bi_feature_vector = []\n",
    "    words = tweet.split()\n",
    "    for i in xrange(len(words) - 1):\n",
    "        word = words[i]\n",
    "        next_word = words[i + 1]\n",
    "        if unigrams.get(word):\n",
    "            uni_feature_vector.append(word)\n",
    "        if USE_BIGRAMS:\n",
    "            if bigrams.get((word, next_word)):\n",
    "                bi_feature_vector.append((word, next_word))\n",
    "    if len(words) >= 1:\n",
    "        if unigrams.get(words[-1]):\n",
    "            uni_feature_vector.append(words[-1])\n",
    "    return uni_feature_vector, bi_feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(len(docs) / float(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) / float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
    "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
    "    for i in xrange(num_batches):\n",
    "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
    "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
    "        labels = np.zeros(batch_size)\n",
    "        for j, tweet in enumerate(batch):\n",
    "            if test_file:\n",
    "                tweet_words = tweet[1][0]\n",
    "                tweet_bigrams = tweet[1][1]\n",
    "            else: \n",
    "                \n",
    "                tweet_words = tweet[2][0]\n",
    "                tweet_bigrams = tweet[2][1]\n",
    "                labels[j] = tweet[1]\n",
    "            if feat_type == 'presence':\n",
    "                tweet_words = set(tweet_words)\n",
    "                tweet_bigrams = set(tweet_bigrams)\n",
    "            for word in tweet_words:\n",
    "                idx = unigrams.get(word)\n",
    "                if idx:\n",
    "                    features[j, idx] += 1\n",
    "            if USE_BIGRAMS:\n",
    "                for bigram in tweet_bigrams:\n",
    "                    idx = bigrams.get(bigram)\n",
    "                    if idx:\n",
    "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
    "        yield features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_idf(X):\n",
    "    \"\"\"\n",
    "    Fits X for TF-IDF vectorization and returns the transformer.\n",
    "    \"\"\"\n",
    "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "    transformer.fit(X)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(csv_file, test_file=True):\n",
    "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
    "            or (tweet_id, sentiment, feature_vector)\n",
    "    Args:\n",
    "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
    "        test_file (bool, optional): If processing test file\n",
    "    Returns:\n",
    "        list: Of tuples\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    print('Generating feature vectors')\n",
    "    with open(csv_file, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if test_file:\n",
    "                tweet_id, tweet = line.split(',')\n",
    "            else:\n",
    "                tweet_id, sentiment, tweet = line.split(',')\n",
    "            feature_vector = get_feature_vector(tweet)\n",
    "            if test_file:\n",
    "                tweets.append((tweet_id, feature_vector))\n",
    "            else:\n",
    "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
    "            utils.write_status(i + 1, total)\n",
    "    print('\\n')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FREQ_DIST_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8d6b471e5697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0munigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNIGRAM_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_BIGRAMS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_n_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBI_FREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBIGRAM_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FREQ_DIST_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1337)\n",
    "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
    "    if USE_BIGRAMS:\n",
    "        bigrams = utils.top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
    "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "    if TRAIN:\n",
    "        train_tweets, val_tweets = utils.split_data(tweets)\n",
    "    else:\n",
    "        random.shuffle(tweets)\n",
    "        train_tweets = tweets\n",
    "    del tweets\n",
    "    print('Extracting features & training batches')\n",
    "    clf = DecisionTreeClassifier(max_depth=25)\n",
    "    batch_size = len(train_tweets)\n",
    "    i = 1\n",
    "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
    "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
    "        utils.write_status(i, n_train_batches)\n",
    "        i += 1\n",
    "        if FEAT_TYPE == 'frequency':\n",
    "            tfidf = apply_tf_idf(training_set_X)\n",
    "            training_set_X = tfidf.transform(training_set_X)\n",
    "        clf.fit(training_set_X, training_set_y)\n",
    "    print('\\n')\n",
    "    print('Testing')\n",
    "    if TRAIN:\n",
    "        correct, total = 0, len(val_tweets)\n",
    "        i = 1\n",
    "        batch_size = len(val_tweets)\n",
    "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
    "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
    "            if FEAT_TYPE == 'frequency':\n",
    "                val_set_X = tfidf.transform(val_set_X)\n",
    "            prediction = clf.predict(val_set_X)\n",
    "            correct += np.sum(prediction == val_set_y)\n",
    "            utils.write_status(i, n_val_batches)\n",
    "            i += 1\n",
    "        print('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
    "    else:\n",
    "        del train_tweets\n",
    "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
    "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
    "        predictions = np.array([])\n",
    "        print('Predicting batches')\n",
    "        i = 1\n",
    "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
    "            if FEAT_TYPE == 'frequency':\n",
    "                test_set_X = tfidf.transform(test_set_X)\n",
    "            prediction = clf.predict(test_set_X)\n",
    "            predictions = np.concatenate((predictions, prediction))\n",
    "            utils.write_status(i, n_test_batches)\n",
    "            i += 1\n",
    "        predictions = [(str(j), int(predictions[j]))\n",
    "                       for j in range(len(test_tweets))]\n",
    "        utils.save_results_to_csv(predictions, 'decisiontree.csv')\n",
    "        print('\\nSaved to decisiontree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(data, batch_size):\n",
    "    \n",
    "    # Preparing Text Data and Label\n",
    "    tweet, label = data['text'], data['sentiment']\n",
    "    \n",
    "    # Tokenizing Text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweet)\n",
    "    sequences = tokenizer.texts_to_sequences(tweet)\n",
    "    tweets_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Model\n",
    "    clf = DecisionTreeClassifier(max_depth=25)\n",
    "    batch_size = len(tweets_pad)\n",
    "    i = 1\n",
    "    \n",
    "    # Data\n",
    "    training_set_X = tweets_pad\n",
    "    training_set_y = label\n",
    "    \n",
    "    # Apply TF-IDF transformation\n",
    "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "    transformer.fit(training_set_X)\n",
    "    tfidf = transformer\n",
    "    training_set_X = tfidf.transform(training_set_X)\n",
    "    \n",
    "    # Traning the model\n",
    "    filepath = \"/Users/BabeChris/Desktop/4th/STA414/A1/DataFest2020/checkpoint/DecisionTree/4cnn-{epoch:02d}-{loss:0.3f}-{mean_squared_error:0.3f}-{val_loss:0.3f}-{val_mean_squared_error:0.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "    print(clf.summary())\n",
    "    \n",
    "    clf.fit(training_set_X, training_set_y, batch_size=batch_size, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
    "    \n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
