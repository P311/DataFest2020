{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(data, wv_from_bin, batch_size=128, kernel_size=5, epoch=8, vec_size=200, loading=False):\n",
    "    \n",
    "    # Preparing Text Data and Label\n",
    "    tweet, label = data['text'], data['sentiment']\n",
    "    label = pd.concat([label==0, label==1, label==-1], axis=1).astype(int)\n",
    "\n",
    "    # Tokenizing Text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweet)\n",
    "    sequences = tokenizer.texts_to_sequences(tweet)\n",
    "    max_length = max([len(s.split()) for s in tweet])\n",
    "\n",
    "    # Preparing Embedding Dictionary\n",
    "    word_index = tokenizer.word_index\n",
    "    tweets_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    embedding_index = {}\n",
    "    words = list(wv_from_bin.vocab.keys())\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            embedding_index[w] = wv_from_bin.word_vec(w)\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    # Preparing Embedding Matrix\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, vec_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    # Preparing Embedding Layer\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index)+1,\n",
    "                            vec_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False\n",
    "                               )\n",
    "    \n",
    "    # Construction LSTM Model Structure\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "    print(model.summary())\n",
    "\n",
    "    # Training Model\n",
    "    model.fit(tweets_pad, label, batch_size=batch_size, epochs=epoch, validation_split=0.1, shuffle=True, callbacks=[reduce_lr])\n",
    "    \n",
    "    # Saving Trained Model\n",
    "    model.save(\"/Users/colinwan/Desktop/DataFest2020/Models/LSTM.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colinwan/Desktop/DataFest2020/Models/ipython/utils.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = \" \".join([word for word in df['text'][i].split()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 29, 200)           2492800   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 29, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 2,669,699\n",
      "Trainable params: 176,899\n",
      "Non-trainable params: 2,492,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/colinwan/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 12483 samples, validate on 1388 samples\n",
      "Epoch 1/32\n",
      "12483/12483 [==============================] - 32s 3ms/step - loss: 0.9068 - accuracy: 0.6112 - val_loss: 0.7793 - val_accuracy: 0.6535\n",
      "Epoch 2/32\n",
      "12483/12483 [==============================] - 35s 3ms/step - loss: 0.8336 - accuracy: 0.6438 - val_loss: 0.7378 - val_accuracy: 0.6780\n",
      "Epoch 3/32\n",
      "12483/12483 [==============================] - 39s 3ms/step - loss: 0.8018 - accuracy: 0.6605 - val_loss: 0.7120 - val_accuracy: 0.6780\n",
      "Epoch 4/32\n",
      "12483/12483 [==============================] - 40s 3ms/step - loss: 0.7807 - accuracy: 0.6677 - val_loss: 0.7314 - val_accuracy: 0.6960\n",
      "Epoch 5/32\n",
      "12483/12483 [==============================] - 41s 3ms/step - loss: 0.7643 - accuracy: 0.6787 - val_loss: 0.6949 - val_accuracy: 0.7032\n",
      "Epoch 6/32\n",
      "12483/12483 [==============================] - 39s 3ms/step - loss: 0.7490 - accuracy: 0.6883 - val_loss: 0.6936 - val_accuracy: 0.7046\n",
      "Epoch 7/32\n",
      "12483/12483 [==============================] - 46s 4ms/step - loss: 0.7330 - accuracy: 0.6940 - val_loss: 0.6943 - val_accuracy: 0.7053\n",
      "Epoch 8/32\n",
      "12483/12483 [==============================] - 42s 3ms/step - loss: 0.7265 - accuracy: 0.6974 - val_loss: 0.6963 - val_accuracy: 0.7053\n",
      "Epoch 9/32\n",
      "12483/12483 [==============================] - 39s 3ms/step - loss: 0.7027 - accuracy: 0.7046 - val_loss: 0.6753 - val_accuracy: 0.7017\n",
      "Epoch 10/32\n",
      "12483/12483 [==============================] - 40s 3ms/step - loss: 0.6866 - accuracy: 0.7111 - val_loss: 0.7010 - val_accuracy: 0.6945\n",
      "Epoch 11/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6814 - accuracy: 0.7132 - val_loss: 0.7028 - val_accuracy: 0.7053\n",
      "Epoch 12/32\n",
      "12483/12483 [==============================] - 38s 3ms/step - loss: 0.6647 - accuracy: 0.7278 - val_loss: 0.6877 - val_accuracy: 0.7061\n",
      "Epoch 13/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6540 - accuracy: 0.7296 - val_loss: 0.6897 - val_accuracy: 0.6996\n",
      "Epoch 14/32\n",
      "12483/12483 [==============================] - 38s 3ms/step - loss: 0.6453 - accuracy: 0.7340 - val_loss: 0.6809 - val_accuracy: 0.7010\n",
      "Epoch 15/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6393 - accuracy: 0.7351 - val_loss: 0.6940 - val_accuracy: 0.6988\n",
      "Epoch 16/32\n",
      "12483/12483 [==============================] - 36s 3ms/step - loss: 0.6346 - accuracy: 0.7380 - val_loss: 0.6832 - val_accuracy: 0.6988\n",
      "Epoch 17/32\n",
      "12483/12483 [==============================] - 38s 3ms/step - loss: 0.6311 - accuracy: 0.7384 - val_loss: 0.6851 - val_accuracy: 0.7039\n",
      "Epoch 18/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6270 - accuracy: 0.7410 - val_loss: 0.6824 - val_accuracy: 0.6981\n",
      "Epoch 19/32\n",
      "12483/12483 [==============================] - 36s 3ms/step - loss: 0.6299 - accuracy: 0.7392 - val_loss: 0.6858 - val_accuracy: 0.6981\n",
      "Epoch 20/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6259 - accuracy: 0.7429 - val_loss: 0.6868 - val_accuracy: 0.6996\n",
      "Epoch 21/32\n",
      "12483/12483 [==============================] - 36s 3ms/step - loss: 0.6318 - accuracy: 0.7391 - val_loss: 0.6850 - val_accuracy: 0.6974\n",
      "Epoch 22/32\n",
      "12483/12483 [==============================] - 38s 3ms/step - loss: 0.6263 - accuracy: 0.7453 - val_loss: 0.6843 - val_accuracy: 0.6981\n",
      "Epoch 23/32\n",
      "12483/12483 [==============================] - 40s 3ms/step - loss: 0.6169 - accuracy: 0.7461 - val_loss: 0.6868 - val_accuracy: 0.7010\n",
      "Epoch 24/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6209 - accuracy: 0.7436 - val_loss: 0.6867 - val_accuracy: 0.7010\n",
      "Epoch 25/32\n",
      "12483/12483 [==============================] - 40s 3ms/step - loss: 0.6209 - accuracy: 0.7429 - val_loss: 0.6870 - val_accuracy: 0.7003\n",
      "Epoch 26/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6296 - accuracy: 0.7409 - val_loss: 0.6866 - val_accuracy: 0.7010\n",
      "Epoch 27/32\n",
      "12483/12483 [==============================] - 41s 3ms/step - loss: 0.6221 - accuracy: 0.7416 - val_loss: 0.6869 - val_accuracy: 0.7003\n",
      "Epoch 28/32\n",
      "12483/12483 [==============================] - 48s 4ms/step - loss: 0.6274 - accuracy: 0.7392 - val_loss: 0.6868 - val_accuracy: 0.7010\n",
      "Epoch 29/32\n",
      "12483/12483 [==============================] - 37s 3ms/step - loss: 0.6232 - accuracy: 0.7390 - val_loss: 0.6868 - val_accuracy: 0.7003\n",
      "Epoch 30/32\n",
      "12483/12483 [==============================] - 35s 3ms/step - loss: 0.6257 - accuracy: 0.7439 - val_loss: 0.6868 - val_accuracy: 0.7003\n",
      "Epoch 31/32\n",
      "12483/12483 [==============================] - 39s 3ms/step - loss: 0.6257 - accuracy: 0.7418 - val_loss: 0.6867 - val_accuracy: 0.7003\n",
      "Epoch 32/32\n",
      "12483/12483 [==============================] - 25s 2ms/step - loss: 0.6233 - accuracy: 0.7406 - val_loss: 0.6867 - val_accuracy: 0.7003\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load Glove Vector\n",
    "    wv_from_bin = utils.load_embedding_model(200)\n",
    "    # Load dataset\n",
    "    data = pd.read_csv('/Users/colinwan/Desktop/DataFest2020/Training Folder/Sentiment.csv')\n",
    "    data = data[['text','sentiment']]\n",
    "    # Replace word with class id\n",
    "    data = data.replace('Neutral', 0)\n",
    "    data = data.replace('Positive', 1)\n",
    "    data = data.replace('Negative', -1)\n",
    "    data = utils.clean_text(data)\n",
    "    # Filter out none alphabet character\n",
    "    data['text'] = data['text'].str.replace('[^A-Za-z ]+', '')\n",
    "    # Run model\n",
    "    LSTM_M = LSTM_Model(data, wv_from_bin, epoch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
